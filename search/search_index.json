{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>This is documentation for a website which applies algorithms to category-based fantasy basketball. The algorithms are described in these papers: </p> <ul> <li>Improving algorithms for fantasy basketball</li> <li>Dynamic algorithms for fantasy basketball</li> <li>Optimizing for Rotisserie fantasy basketball</li> </ul> <p>Please note that these algorithms are based on a simplified model of fantasy basketball, ignoring many practical considerations, and there is no guarantee that using them will lead to success. Don't expect to automatically win your league with the algorithms or even to have a better shot than anyone else. The intent of the papers is just to start exploring the math underlying fantasy basketball, and the intent of the website is to have fun playing around with that math  </p> <p>Source code is available here.</p>"},{"location":"#category-based-fantasy-basketball","title":"Category-based fantasy basketball","text":"<p>In fantasy leagues, \"managers\" draft teams of real players before the season begins. E.g. I take Victor Wembanyama with my first pick, then you take Nikola Jokic, someone else takes Luka Doncic, and so on. At the end of the draft everyone has a 13-player team. </p> <p>Throughout the season, teams accumulate statistics based on their real players' performances. E.g. if I have Wembanyama on my team and he gets four blocks in a game, then my team gets four blocks. </p> <p>In category-based leagues, which most fantasy basketball leagues are, the important thing is winning individual categories. Having more of a category means you win it, e.g. if my team gets 50 blocks and your team gets 30 blocks, I get a \"fantasy point\" for winning blocks. All of the tracked categories, usually eight or nine in total, are scored this way. Generally opponents are rotated weekly, and teams try to accumulate as many fantasy points as they can across all the opponents they face throughout a season.</p> <p>This simple scoring system belies deceptively tricky mathematics. There is no obvious way to compare the values of categories to each other, making it difficult to quantitatively decide which player is best to take. The motivating force behind the papers is untangling this conundrum and applying rigor to the process of evaluating players. </p>"},{"location":"#the-papers","title":"The papers","text":"<p>Broadly, the papers formulate category-based fantasy basketball as a family of math problems, and provide methods for selecting players based on those formulations. The methods take projections of player performance as an input. </p> <p>The first paper looks at so-called \"static\" systems, which estimate player value in a vacuum. It provides a mathematical justification for Z-scoring, the traditional metric used by fantasy basketball analysts, and shows a way to improve it. The improved metric, dubbed G-scoring, is used by the website for describing player value statically. See also the G-score section of documentation. </p> <p>The second paper discusses how to evaluate players with full context taken into account. The most difficult aspect of context to incorporate is selections from later draft rounds, because nobody can be exactly sure which players will be taken by which manager in the future. The paper's solution is an algorithm called H-scoring which optimizes a heuristic approach to future draft picks for each candidate player. Players can then be ranked according to how well the team would be expected to perform if they were taken, given the appropriate follow-up in later draft rounds. H-scores are the default way to evaluate players on the website. See also the H-score section of documentation. </p> <p>The third paper approaches the trickiest format, Rotisserie or \"Roto\". Instead of weekly matchups, Roto has one season-long scoring period in which all managers compete against all other managers. The manager who wins the most fantasy points in that one scoring period wins the league. Since this format is so different, adapting H-scoring to it requires another layer of mathematical scaffolding. See also the Roto section of documentation. </p>"},{"location":"#extensions","title":"Extensions","text":"<p>The website extends the papers in a few ways. </p> <p>One is that it provides methods for auction drafts in addition to typical snake drafts. The papers do not discuss auctions directly, but the snake draft methods can be adapted to work for auctions. </p> <p>Another is that it adjusts implicit projections during drafts. This is a practical consideration which is very useful during real drafts. </p> <p>It also provides some related analyses for leagues that are already underway via \"Season Mode\". The analyses evaluate player acquisitions and trades. </p>"},{"location":"auctions/","title":"Auction Mode","text":"<p>Auction drafting is more complicated than snake drafting because auction drafters need to decide how much to bid on players, not just which players to take. This level of complexity makes strategizing perfectly for an auction even more impossible than it is for a snake draft. </p> <p>Still, quantitative analysis can be helpful in the auction context. In particular, it can be used to benchmark player values in terms of auction dollars. </p> <p>The auction mode of this website implements some basic methods for converting G-scores and H-scores into dollar values. It also makes an adjustment to player values that is unique to the auction context called SAVOR.</p>"},{"location":"auctions/#using-auction-mode","title":"Using auction mode","text":"<p>When the selected mode is 'auction', the website will provide analysis for either synthetic or live auctions. </p>"},{"location":"auctions/#manual-entry","title":"Manual entry","text":"<p>Player selection information can be entered into the editable table. Analysis will not begin until the 'Lock in' button is pressed; this is to allow for multiple updates without running H-scoring every time. </p>"},{"location":"auctions/#live-connection","title":"Live connection","text":"<p>For some reason, Yahoo's API does not return anything for auctions until a few minutes after the auction has started. Because of that, the displayed values may be the default values for the first few picks. </p> <p></p> <p>Once the 'Auction has not yet begun' message is gone, information is being received from Yahoo. </p>"},{"location":"auctions/#quantifying-auction-value","title":"Quantifying auction value","text":"<p>A well-known heuristic for quantifying auction value is described in many places including this article from rotowire. For reference, it is</p> <ol> <li>Calculate the replacement-level score. That is, if 156 players will be chosen, the 157th-highest score is the replacement value</li> <li>Adjust all scores by subtracting out the replacement-level value. If this would make a score go below zero, set it to zero instead</li> <li>Calculate the sum of scores above replacement. This is the total amount of real value available in the auction</li> <li>Divide the total number of dollars available by the total amount of real value available. This yields a conversion rate from score above replacement to dollars</li> <li>Multiply each players' score above replacement with the conversion rate calculated in the previous step. The result is each players' auction value</li> </ol> <p>This process ensures both that players' dollar values proportional to their values over replacement, and that the total of all players' dollar values are equal to the total amount of $ available. </p> <p>Auction mode uses this process to quantify player value in a few ways.</p>"},{"location":"auctions/#converting-g-score-value-to-dollar-value","title":"Converting G-score value to dollar value","text":"<p>Two kinds of dollar values are presented for G-scores</p> <p></p> <p>G-score-based $ values in a synthetic draft context</p> <p>'Orig. $' value, or original value, is the same as the auction value heuristic described above. Original values do not change during auctions, and can be helpful as objective benchmarks that quantify how good deals are in the abstract. </p> <p>'Gnrc. $' value, or generic value, is a variant which is recomputed as players are taken and the amount of available money decreases. For example if two players out of 156 have been taken for $200 total, those two players are removed from the list, the replacement-level value becomes the 155th-highest score, and 200 dollars are removed from the amount of total dollars available. The same process as for original value is then applied using the modified inputs. Generic value may be useful strategically because it reflects whether other managers have been under- or over-spending. E.g. if managers have been underspending, it implicitly takes into account the fact that some managers have excess money and will be able to pay more for remaining players. </p>"},{"location":"auctions/#converting-h-score-value-to-dollar-value","title":"Converting H-score value to dollar value","text":"<p>H-scores are probabilities, not general values. They are converted into dollar values with two steps</p> <ol> <li>It is estimated how much money it would take to improve winning chances by the same amount as taking the player</li> <li>Those monetary estimates are refined into dollar values with the auction value heuristic as described previously</li> </ol> <p></p> <p>H-score-based $ values in a synthetic draft context</p> <p>The original and generic values are based on players' H-scores converted to monetary estimates for the first pick of the auction, with no players taken and no cash spent. Those are not context-dependent so they stay the same for the whole auction. The lack of context also means that they are imprecise approximations of real player value.</p> <p>Like for G-scores, the original values are processed once with the auction value heuristic and stay the same throughout the auction. </p> <p>For generic values, the underlying step 1 estimates are not changed, but the step 2 process is adjusted for the number of players remaining etc. That is, if a player was estimated to be worth $30 originally, that number will continue to be plugged in as a value to the auction value heuristic process. The auction value heuristic process will be slightly different because players have been taken and cash has been spent. </p> <p>H-scoring is also run with with the updated context for the drafter in question. Those monetary estimates become 'Your $' after refinement through the auction value heuristic. The difference between 'Your $' and 'Gnrc. $' highlights players which are more or less valuable to the drafter in question than they are to a generic drafter. </p>"},{"location":"auctions/#the-savor-adjustment","title":"The SAVOR adjustment","text":"<p>After the previously described processing for H-score and G-score dollar values, the website makes an additional adjustment called SAVOR. It is reflected in all of the displayed dollar values. </p> <p>SAVOR stands for Streaming-Adjusted Value Over Replacement. It adjusts for the fact that the lowest-ranking players are highly likely to be shuffled around over the course of the season through waiver wires and free agency, so it is not worth spending much money on them, even if theoretically they are projected to be somewhat more valuable than their alternatives. This is a known concept in the fantasy basketball community- for example it is referenced in this reddit thread.</p> <p>Details of the mathematical model behind the SAVOR adjustment are included in the appendix of an old version of the first paper. It was removed from the most recent version because it was not topical. </p> <p></p> <p>SAVOR takes an input parameter, \\(S_{\\sigma}\\). It controls the degree to which players are expected to move up and down across the season according to the SAVOR model. Its default value is sourced by vibes- different values may be just as or more reasonable. </p>"},{"location":"drafts/","title":"Draft Mode","text":"<p>H-scores and G-scores are designed directly for drafts, so they can be applied to drafts without modification.</p>"},{"location":"drafts/#using-draft-mode","title":"Using draft mode","text":""},{"location":"drafts/#manual-entry","title":"Manual entry","text":"<p>With manual entry, draft picks are entered through the website. </p> <p></p> <p>The 'Lock in selection' button puts the player shown in the dropdown into the next draft slot. Picks go in a snake order and cannot be skipped. </p> <p>The default order in which players are listed in the dropdown is by total G-score. The top player on the list is the default selection. So if 'Lock in selection' is pressed multiple times in succession, available players are taken in G-score order. </p> <p>The table below the player selection dropdown shows which players have been taken by which drafters. </p> <p>Tables for G-scores and H-scores of candidate players are shown when the 'Run algorithm' button on the right is pressed. </p> <p></p> <p>The G-score table is available immediately because it is static; the H-scoring table will take some time to fully update. The H-score details tab will also be available, synched to the current display of the H-score table and updated as the algorithm runs. </p> <p></p>"},{"location":"drafts/#live-connection","title":"Live connection","text":"<p>With a live connection, draft selections are provided by the platform. The entire screen becomes a view for candidate evaluation. </p> <p></p> <p>The 'Refresh Analysis' button fetches new information on draft picks from the platform and re-runs H-scoring. </p>"},{"location":"gscores/","title":"G-scores","text":"<p>It is well-understood that player value in category leagues is dependent on context. No single number, independent of circumstances around team, opposition, etc. can ever fully define a player's value. However, that has not stopped fantasy enthusiasts from designing and applying so-called 'static' ranking systems. Despite their limitations in theory, they are useful in practice because they are simple and convenient. One ought not let the perfect get in the way of the good. </p> <p>The website uses G-scores as a measure of static value. G-scores are a variant of the traditional Z-score metric, as described in my first paper. See also the justification section of this page for a relatively simple explanation. </p>"},{"location":"gscores/#g-score-table","title":"G-score table","text":"<p>During auctions and drafts, a tab will be available with the G-score table for available players. The G-score table shows available players ordered by total G-score during drafts and auctions. It includes the categorical components of G-scores as well.</p>"},{"location":"gscores/#team-table","title":"Team table","text":"<p>The team table shows the G-scores of players already chosen for a team, and their totals. The totals show how the team is doing in general, though one should keep in mind that non-turnover categories tend to have high values during early rounds because only the strongest players are being taken. </p>"},{"location":"gscores/#justification","title":"Justification","text":"<p>Warning- math  </p>"},{"location":"gscores/#what-are-z-scores","title":"What are Z-scores?","text":"<p>Fantasy basketball has a standard way of quantifying player value across categories, called 'Z-scoring', and it is used to make objective rankings of players. </p> <p>In a stats 101 class, Z-scores are what happens to a set of numbers after subtracting the mean (average) signified by \\(\\mu\\) and dividing by the standard deviation (how \u201cspread out\u201d the distribution is) signified by \\(\\sigma\\). Mathematically, \\(Z(x) = \\frac{x - \\mu}{\\sigma}\\).</p> <p>Z-scores in the fantasy context are essentially the same thing, with a few minor modifications. They take a player's expected performance in a category, subtract out the average from the paper pool, and divide by the standard deviation. </p>"},{"location":"gscores/#justifying-z-scores","title":"Justifying Z-scores","text":"<p>Consider this problem: Team one has \\(N-1\\) players randomly selected from a pool of players, and team two has \\(N\\) players chosen randomly from the same pool. Which final player should team one choose to optimize the expected value of categories won against team two, assuming all players perform at exactly their long term mean for a week?</p> <p>The difference in category score between two teams indicates which team is winning the category and by how much. Randomly selecting the \\(2N -1\\) random players many times gives a sense of what team two's score minus team one's score will be before the last player is added. See this simulation being carried out for blocks below with \\(N=12\\)</p>    Your browser does not support the video tag.  <p>You may notice that the result looks a lot like a Bell curve even though the raw block numbers look nothing like a Bell curve. This happens because of the surprising \"Central Limit Theorem\", which says that when adding a bunch of random numbers together, their sum always ends up looking a lot like a Bell curve. </p> <p>The mean and standard deviation of the Bell curves for category differences can be calculated via probability theory. Including the unchosen player with category average \\(m_p\\) - The mean is \\(m_\\mu - m_p\\) - The standard deviation is \\(\\sqrt{2N-1} * m_\\sigma\\) (The square root in the formula comes from the fact that \\(STD(X + Y) = \\sqrt{STD(X)^2 + STD(Y)^2}\\) where \\(STD(X)\\) is the standard deviation of \\(X\\))</p> <p>When the category difference is below zero, team one will win the category</p> <p>The probability of this happening can be calculated using something called a cumulative distribution function. \\(CDF(x) =\\) the probability that a particular distribution will be less than or equal to \\(x\\). \\(CDF(0)\\), then, is the probability that the category difference is below zero and team one wins (ignoring ties). </p> <p>The \\(CDF\\) of the Bell curve is well known. The details of how to apply it to this case are somewhat complicated, but we can cut to the chase and give an approximate formula </p> \\[ CDF(0) = \\frac{1}{2}\\left[ 1 + \\frac{2}{\\sqrt{\\pi}}* \\frac{- \\mu }{ \\sigma} \\right] \\] <p>\\(\\mu\\) and \\(\\sigma\\) for the standard statistics are already known. Substituting them in yields</p> \\[ CDF(0) = \\frac{1}{2}\\left[ 1 + \\frac{2}{\\sqrt{(2N-1) \\pi}}* \\frac{m_p \u2013 m_\\mu}{m_\\sigma} \\right] \\] <p>Hey look, that's the Z-score! this equation shows that an extra point of Z-score translates into an increased probability of winning the category in a consistent way. </p>"},{"location":"gscores/#extending-to-g-scores","title":"Extending to G-scores","text":"<p>To justify Z-scores, it was assumed that each player would perform precisely at their long-term mean. But that was a bad assumption, because players don't perform consistently week-to-week. The question can be improved by assuming that players are chosen randomly and their performances are chosen randomly too. </p> <p>Below, see how metrics for blocks change when weekly performance of the top \\(156\\) players are sampled, instead of just their averages </p>    Your browser does not support the video tag.  <p>Although the mean remains the same, the standard deviation gets larger. This makes sense, because week-to-week \"noise\" adds more volatility, which is reflected in the additional \\(m_\\tau\\) term. Note that the new standard deviation is \\(\\sqrt{m_\\sigma^2 + m_\\tau^2}\\) rather than \\(m_\\sigma + m_\\tau\\) because of how standard deviation aggregates across multiple variables, as discussed previously</p> <p>Also keep in mind that for Rotisserie, the uncertainty is in season-long performance, rather than week-by-week variance. </p> <p>Substituting the new standard deviation into the Z-score equation creates G-scores. they are </p> \\[ \\frac{m_p \u2013 m_\\mu}{\\sqrt{m_\\sigma^2 + m_\\tau^2}}  \\]"},{"location":"gscores/#calculation","title":"Calculation","text":"<p>The coefficients for G-scores used by the website were calculated based on real historical data, as shown in the paper. One should not that real week-to-week variance in historical data is not necessarily the same thing as forecasted variance for future weeks, so the week-to-week variance factor may be too strong or too weak in practice. </p>"},{"location":"hscores/","title":"H-scores","text":"<p>H-scoring is a framework introduced in the second paper for dynamic player selection. In short, for each candidate player, it optimizes for future draft pick strategy in terms of category weightings and position allocations, and estimates performance based on those strategies. This allows the algorithm to understand general drafting strategy, including the concept of punting (strategicially sacrificing categories). See the optimization section for some mathematical detail on how it does that without the academic rigor of the paper. Because of its ability to adapt to drafting circumstances, H-scoring arguably offers a more compelling and logical starting point for draft strategy than G-scoring. </p> <p>The website includes a module which performs the H-score algorithm programatically. </p>"},{"location":"hscores/#parameter-inputs","title":"Parameter inputs","text":"<p>The H-scoring algorithm has three input parameters: \\(\\omega\\), \\(\\gamma\\), and the number of iterations which are configurable by the user.</p> <p>\\(\\omega\\) and \\(\\gamma\\) control how the H-scoring algorithm thinks about the landscape of player statistics that it will have to choose from in the future. Roughly, when \u03c9 is high, the algorithm punts more. The default values were configured based on what worked well in testing. </p> <p>The H-scoring algorithm runs for the specified number of iterations. Additional iterations increase the precision of H-scoring, at the cost of longer computation. In practice thirty iterations, the default, works reasonably well.  </p>"},{"location":"hscores/#h-score-table","title":"H-score table","text":"<p>The H-score table for candidate evaluation lists players in order of their H-score rank, along with category-level detail. </p> <p></p> <p>Top Each Category H-scores for the first pick, 2024-25 season</p> <p>H-scores change as the algorithm runs. Rendering the results for every iteration would slow down the site, so the table is refreshed once every fifteen iterations. Also, only the top thirty players by H-score are shown until the last iteration, at which point all players are shown. </p> <p>One might note that Giannis Antetokounmpo ranks highly by H-score. Fantasy veterans will be familiar with Giannis for being undervalued by static ranking systems like overall Z-score, because he has strong value only with the strategy of sacrificing the Free Throw % category. The H-scoring algorithm is able to account for this and show a more realistic value for Giannis. </p>"},{"location":"hscores/#category-level-h-scores","title":"Category-level H-scores","text":"<p>Category-level H-scores are an in important part of the H-scoring process. Unlike with G-scores, they are not direct reflections of the candidate player's characteristics. Instead, they show what the H-scoring algorithm expects the average win rate against all opponents will be, assuming the candidate player is taken. H-scoring calculates those expectations based on not just the characteristics of the candidate player, but also on previously chosen players and potential future picks. The statistics of future picks are estimated based on H-scoring's preferred strategy for future picks.</p> <p>Because other factors are taken into account, the categorical strengths and weaknesses presented in the H-score table are often quite different from those of the candidate players. For early picks, the most important factor is the strategy for future draft picks. For example, Shai Gilgeous-Alexander's row above for the first pick in the draft shows a very low probability of winning the Rebound category, despite SGA being a decent rebounder himself. This is because H-scoring's preferred strategy with SGA involves deprioritizing rebounds with future picks.</p> <p>In later draft rounds, the importance of previously chosen players increases and the importance of the strategy for future picks decreases. Also, the strategy for future picks tends to become more stable across players, since the direction of the team is already decided. So categorical H-scores tend to become consistent across players. </p> <p></p> <p>Top H-scores for a round seven pick in a mock draft, with relatively stable scores across categories. Each Categories, 2024-25</p>"},{"location":"hscores/#overall-h-scores","title":"Overall H-scores","text":"<p>The overall H-score is both the metric that H-scoring is trying to optimize with its future draft pick strategy, and the one used to rank players. It is based on the category-level H-scores. </p> <p>For Each Categories scoring, it is defined as the average expected win rate across categories. </p> <p>For Most Categories, the average expected win rate is a poor proxy for success. A team that wins five out of nine categories 100% of the time and always loses the others is better for that format than one that wins each category 60% of the time, despite having a lower average expected win rate (56% vs 60%). For that reason, switching the format to Most Categories switches the definition of the overall H-score to the probability of winning a majority of categories (assuming they are independent for the sake of making the calculation less intensive).</p> <p></p> <p>Top Most Category H-scores for the first pick, 2024-25 season</p> <p>The table above is based on the same dataset as the Each Category version. The numbers, and the order of players, are different because they use the Most Categories objective instead of the Each Category objective. With Most Categories scoring, the algorithm is even more incentivized to punt, and tends to do so to a more extreme degree. Players that benefit strongly from punting like Giannis also end up ranking better (third instead of fifth).</p> <p>Rotisserie is another degree more complicated than Most Categories. See the Roto section for details</p>"},{"location":"hscores/#h-score-details-tab","title":"H-score details tab","text":"<p>The main H-score table gives only indirect insight into the strategies that H-scoring wants to use with each candidate player. The H-score details tab explains the strategy for individual players more directly. It is broken down into two parts: expectations, and future strategy.</p>"},{"location":"hscores/#expectations","title":"Expectations","text":"<p>Expectations for a team with Giannis as the first pick based on Dyson Daniels as the second, Each Category 2024-25</p> <p>The first row is the same as that from the main H-scoring table, included for convenience. </p> <p>The table below breaks down the components of the team, in terms of G-scores vs. the average of other teams. 'Current diff' represents the G-score differential for the draft so far, including players already drafted in the current round and excluding the candidate player. Teams that have not made their pick for the round are filled in with an estimate of the statistics of their next player. So in this case above, 'Current diff' represents other teams' first two picks vs. Giannis, with estimates for other teams that have only drafted one player. 'Future player diff' is the expected difference between future picks made by the drafter and those made by other teams, based on the strategy adopted by H-scoring. In this case the G-score for Free Throws is heavily negative because the algorithm wants to punt it with future picks. 'Current diff' plus the candidate player plus 'Future player diff' equals the total differential versus other teams, which H-scoring uses to calculate win probabilities.  </p> <p>The ranks show how the candidate player ranks as a pick in this situation, both by H-score and G-score. This can also be seen through the H-score or G-score main tables, and is included here as a convenience. </p>"},{"location":"hscores/#future-strategy","title":"Future strategy","text":"<p>The future strategy tab shows H-scoring's strategy for future picks.</p> <p></p> <p>Strategies for future draft picks based on having Giannis and Dyson Daniels already, Each Category 2024-25</p> <p>The category weightings displayed in the first row are based on H-scoring's internal model of how drafting works. It assumes that the drafter will use those weights exactly for candidates going forward, and it also assumes that those weights will have a certain influence on the aggregate statistics of future picks. One might note that in the case above, the weight for Free Throws is surprisingly high, despite the obvious fact that the algorithm is deprioritizing the category heavily. The reason for this is that in general, H-scoring does not think it needs to adjust weights all that much in order to skew the available candidates to the categories it wants. </p> <p>The flex position allocations show how the algorithm expects to use its flex spots, which can take players of multiple positions. This is relevant because the algorithm understands that different positions have different statistical tendencies. In the example above, the algorithm is leaning heavily on taking Power Forwards and Centers with its flex spots, likely because they tend to have poor Free Throw rates, and that synergizes with the strategy of punting Free Throws. </p> <p>The algorithm also has some leeway in how it arranges players already taken in terms of position, freeing up different positions to take with future draft picks. The roster assignment row shows what the algorithm is thinking in this regard. In the example above, it is choosing to categorize Daniels as a SF, likely because it does not want to take more SFs in general. </p>"},{"location":"hscores/#h-scoring-tendencies","title":"H-scoring tendencies","text":"<p>The traditional fantasy basketball strategy is to punt some categories and compete in others. The H-scoring algorithm tends to behave the same way. </p> <p></p> <p>Image from the paper</p> <p>This image from the paper shows how the H-scoring algorithm actually performed on a category level in a simulation. It shows that H-scoring often punted categories, though the punts were not always total. It often preserved some chance of winning punted categories. For the categories it did not punt, it tried to be highly competitive in them without going overboard. </p>"},{"location":"hscores/#optimization","title":"Optimization","text":"<p>Warning- math  </p> <p>H-scoring needs some way of making decisions about category prioritization and position allocation. It makes these decisions through an optimization process, specifically gradient descent. Each iteration of the algorithm is an iteration of gradient descent. </p> <p>The gradient of a function is derivative over multiple dimensions. As an extremely simple example, the gradient of \\(x - 3y\\) is \\(1\\) in the x direction and \\(-3\\) in the y direction. The gradient gives a hint at which direction the function can be minimized or maximized. The idea of gradient descent is to step in the direction of the gradient (or the opposite direction) to try to find a point which minimizes or maximizes the function. </p> <p> </p> <p>In order for gradient descent to work, the function to be mazimized or minimized must be differentiable. This motivates the structure of the model behind H-scoring. </p> <p>Roughly, the function for H-scoring has three components: category strength expectations, category-level victory probabilities, and the outer-level objective function. The decisions made by the algorithm impact the category strength expectations, which in turn impact category-level victory probabilities, which in turn impact the outer-level objective function. The total gradient relative to an input decision is the gradient of all three steps relative to the previous, multiplied together.</p>"},{"location":"hscores/#category-strength-expectations-weights","title":"Category strength expectations: weights","text":"<p>The heart of the H-scoring algorithm is its treatment of future draft picks. Essentially, it assumes that it will be able to choose from a small slate of available players whose statistical profiles are random, conditioned on the scores being equal in terms of total G-score. Using some mathematical estimations, it can calculate the expected deviation from the average for each category based on weighted preference for categories. The math behind this is quite complicated... </p> <p></p> <p>Disgusting! </p> <p>But the basic intuition is simple. The more weight it assigns a category, the higher its team's expected value for that category will be. This allows the algorithm to understand that it can prioritize or deprioritize categories with future picks. </p>"},{"location":"hscores/#category-strength-expectations-position-allocation","title":"Category strength expectations: position allocation","text":"<p>The algorithm can also influence expected category strengths by deciding how it expects to divvy up future draft slots in terms of positions. </p> <p>Position allocations are binary, and therefore their effects cannot be differentiated. In order to avoid costly mized-integer optimization, H-scoring treats position allocation as a small sub-problem and solves it independently. Before each round of gradient descent, the algorithm estimates how much it wants a player of each position with future picks, by multiplying the gradients relative to categories (which encode how much the algorithm has to gain from improving in those categories) by the average value of a player of that position. It assumes flex spots are slightly more valuable than the best base position. The algorithm is then allowed to assign previously chosen players to various slots, to free up future position slots for the positions it wants to take players in. </p> <p>This kind of problem is called an assignment problem, because slots are being assigned to players. Its reward structure can be encoded into a matrix as shown: </p> <p></p> <p>Example of an assignment matrix from the paper. Previously chosen players accrue rewards of zero because they have already been chosen. Their reward is set to negative infinity for positions they cannot be assigned to so that the algorithm knows it cannot make those assignments</p> <p>There are fast algorithms available for assignment problems, such as the Hungarian algorithm (though H-scoring actually uses a faster variant).</p> <p> </p> <p>After the sub-problem is solved, the algorithm will have a strategy for how what positions it wants to prioritize with future picks- e.g. two guards, one shooting guard, and one center. It then has a degree of freedom in how it allocates the guards between SG and PG, which it optimizes jointly with the decision on how to weigh categories. All of these deviations are added together to produce a model for decision-based adjustments to categorical strengths. </p>"},{"location":"hscores/#category-level-victory-probability","title":"Category-level victory probability","text":"<p>Victory probabilities can be estimated with Normal CDFs based on team-level category averages, thanks to the Central Limit Theorem. Normal CDFs are differentiable; their gradients are Normal PDFs. </p> <p>That means that the algorithm implicitly \"cares\" about categories according to a Normal PDF of category strength. Normal PDFs are thick in the middle and thin on the side, so the algorithm naturally cares most about categories for which it has neutral strength. </p> <p></p> <p>A Normal distribution, from Wikipedia </p> <p>This makes intuitive sense and explains why the algorithm behaves like it does, punting some categories and trying to compete in others without going overboard. If a team is already bad at a category, it might make sense to abandon that category and punt it, since the team is likely to lose it anyway. That creates a snowball effect- the team gets worse and worse at the category. If a team is extremely strong in a category, it doesn't need the extra help there, and prioritizing it is not important. That does not create a snowball effect, since as the team gets weaker in the category, H-scoring starts to prioritize it more. That means the algorithm is incentivized to put categories into one of two buckets- punting, or competing without going overboard.</p>"},{"location":"hscores/#outer-level-objective-function","title":"Outer-level objective function","text":"<p>The outer-level objective function is the function that the algorithm is trying to directly maximize. For H-scoring, it is the overall H-score. </p> <p>For Each Category, the objective function is just the sum of probabilities of winning each category. That is relatively simple to optimize, since the overall gradient is just the sum of all the individual gradients.</p> <p>For Most Categories scoring, the objective function is the probability of winning a majority of categories, which is more complicated. Fortunately, this operation turns out to be differentiable, so it can be optimized via gradient descent. The gradient is the same as for Each Categories except multiplied by a 'tipping point' probability, which is the likelihood that any given category will end up being decisive. </p>"},{"location":"hscores/#limitations","title":"Limitations","text":"<p>H-scoring has numerous limitations. Some of the most major are </p> <ul> <li>H-scoring is reliant on a single set of projections which may differ from the beliefs of other managers. Assuming its projections are correct, the algorithm can become overconfident and assess its own team as being so strong that the only way to improve it is to \"un-punt\" a category. This can lead to late round picks which run counter to the build of a team. The website does have a way to mitigate this, to a degree- see the page on the Bayesian strength adjustment</li> <li>The optimization process for H-scoring only considers one strategy profile. It does not consider how robust players are to different strategy profiles, which may be relevant because circumstances can change during a draft, and the algorithm might switch strategies drastically. </li> <li>The internal logic of H-scoring does not understand that other drafters may also be trying to punt categories. This will lead to inaccurate projections of other teams, and therefore inaccurate projections of expected win rates</li> <li>H-scoring does not model category variance based on players. Instead, it assumes that week-to-week variance is the same for all matchups. This is not always accurate, especially when a team is punting a category</li> <li>H-scoring's model for what sorts of players will be available in the future is simplified, and may fail to properly account for individual players with exceptional profiles</li> <li>H-scoring does not take into account the effect of streaming players, trading, etc. These all may add additional strategic considerations </li> </ul>"},{"location":"parameters/","title":"Parameters","text":"<p>Various calculations performed by the website take in parameters that are configurable by the user. They dictate how injury risk is handled, how aggressively the algorithm will punt, and more. </p>"},{"location":"parameters/#player-stat-parameters","title":"Player stat parameters","text":""},{"location":"parameters/#injury-handling","title":"Injury handling","text":"<p>Projections generally include forecasts of how many games each player will play during the season, but incorporating them into player valuations is not entirely straightforward. </p> <p>Typically, player valuations are presented in two ways: per-game values and season total values. Per-game values exclude the missing games, while season total values include them as all zeros. The website allows granular control of the spectrum between those two perspectives, plus an additional correction for players being substituted out for replacement players. </p> <p></p> <p>The first factor, \u03c5, scales injury rates on a spectrum between per-game value and season total values. For example if \u03c5 is \\(0.4\\) and a player is expected to be injured 10% of the time, that injury rate is adjusted to 4%, and the player's volume projections are multiplied by 96%. A \u03c5 of \\(0\\) is equivalent to per-game totals, and a \u03c5 of 1 is equivalent to season total projections. The argument for setting \u03c5 to \\(1\\) is that the correct expected value of real player production fully accounts for the probability of injury. The counter-argument is that teams need to be somewhat lucky to have any shot at competing for a championship, so it makes sense for them to strategize with the assumption that their injury luck is reasonably good. The default value for \u03c5 is \\(1\\), equivalent to season total values.</p> <p>The second factor, \u03c8, controls an adjustment for replacement players. It is assumed that when a player misses a game, they will be replaced by a replacement-level player for that game \u03c8 of the time, and that is incorporated into projections after they have been adjusted for injury rates. A replacement-level player has the total G-score value of the \\(N\\)th-highest player, spread across categories, where \\(N\\) is the number of players in the league.  So continuing the previous example, if \u03c8 is \\(0.75\\), then 3% times a replacement player's value is added to the player's projection. The right value for \u03c8 depends on a league's IR rules and how active managers will be in replacing their injured player. It defaults to \\(0.8\\).</p>"},{"location":"parameters/#chi-factor","title":"Chi factor","text":"<p>For Rotisserie leagues, an additional parameter called \u03c7 is required.</p> <p></p> <p>\u03c7 controls the estimate of uncertainty in pre-season projections. See the relevant section on Rotisserie for more detail. </p>"},{"location":"parameters/#auction-noise","title":"Auction noise","text":"<p>For auction drafts, an additional parameter called \\(S_\\sigma\\) is required.</p> <p></p> <p>\\(S_\\sigma\\) quantifies the standard deviation of dollar values for players throughout a season, which is important for the SAVOR adjustment. Roughly speaking it controls the degree to which low-level players are down-weighted for potentially becoming irrelevant. </p>"},{"location":"parameters/#bayesian-strength-adjustment","title":"Bayesian strength adjustment","text":"<p>The \\(\\beth\\) parameter controls the influence of the Bayesian strength adjustment. Higher values of \\(\\beth\\) more aggressively regress the strength of the team towards the average. </p>"},{"location":"parameters/#h-score-parameters","title":"H-score parameters","text":"<p>The H-score parameters are inputs to the H-scoring procedure. They control the degree to which the algorithm is incentivized to punt, and how long the algorithm runs for. See the documentation on H-scoring for more information. </p>"},{"location":"parameters/#trade-parameters","title":"Trade parameters","text":"<p>The trade parameters limit which potential trades are considered for suggestions. See the section on trade suggestions for the implications of these parameters.</p>"},{"location":"parameters/#position-requirements","title":"Position requirements","text":"<p>H-scoring assumes that either a team conforms to a flexible-enough position structure, or it is invalid. The exact position structure that it checks against is configurable by the user.</p> <p></p> <p>Flex slots like Utilities and Guards can be filled by players of multiple different positions. Bench slots are filled by the last players drafted, and do not count for the H-scoring calculations. </p> <p>It is important to note that this position structure should not necessarily be the same as the league's position structure. The league position structure might include bench slots which players can be moved in and out of on a day-to-day basis to make their games count. Players sitting on that kind of bench do matter, so long as the team is balanced enough in terms of position to accomodate all the players who are active on a given day. Those bench slots should be included as Utilities, or perhaps extra Guards or Forwards to ensure adequate balance. The proper configuration will depend on the rules of a league and some degree of personal preference. </p>"},{"location":"projectionadjustment/","title":"Adjusting projections with a Bayesian prior","text":"<p>Warning- math  </p> <p>H-scoring as described by the papers is fully reliant on a single set of projections. If a drafter takes a player it projects to be a poor performer highly, the algorithm will not \"doubt itself\" and consider the possibility that its projections for that player are too low. It will assume that pick was a poor choice and the drafter who took it will have a bad team. </p> <p>This inability to doubt itself makes the algorithm overconfident, believing that its own team is very strong, when its own projections are not necessarily better than those implicitly used by other drafters. As a practical matter this can lead the algorithm to think its team is so strong that the only way to improve is to \"un-punt\" categories it has given up on, which is probably a bad idea in practice. </p> <p>The papers assume that player projections are all known and agreed upon by all the drafters, so they don't address this issue. However, it is so important in practice that I've added a module specifically to address it. </p>"},{"location":"projectionadjustment/#the-adjustment","title":"The adjustment","text":"<p>An adjustment is made to the algorithm's assessment of its team's strength for any pick after the first. </p> <p>Say that \\(w\\) is a vector of the algorithm's naive guess at how likely it is to win each category, before performing gradient descent to optimize a future strategy. Corrected versions are calculated as</p> \\[ w^* = \\left[ I_{n \\times n} + \\frac{\\beth}{ n^2}\\mathbf{1}_{n \\times n}  \\right]^{-1} \\left[ w + \\frac{\\beth}{2n} \\mathbf{1}_n \\right ] \\] <p>Where \\(n\\) is the number of categories and \\(\\beth\\) is a parameter. The intuition on what this expression is doing is not immediately clear, but some intuition can be gleaned from the justification in the following section. </p> <p>These corrected win rates are then used to reverse engineer an adjusted expectation of the team's current strength, like so: </p> \\[ x^* = \\text{CDF}^{-1} \\left( w^* \\right) \\] <p>This way, as the punting strategy changes, the algorithm's opinion of its own team does not change. Re-adjusting the win rates every for every iteration of the algorithm based on the current expected win rates would implicitly change the algorithm's opinion of its pre-existing team based on its strategy for the future, which does not make much sense. </p>"},{"location":"projectionadjustment/#justification","title":"Justification","text":"<p>Say that we have prior expectations that </p> <ul> <li>Our average win rate across all categories is approximately 50%, with Normally distributed error. </li> <li>Our guesses for how often we will win a category are unbiased, but have some Normally distributed error. </li> </ul> <p>This information provides a Bayesian framework for re-calculating adjusted category-level win rates. </p> <p>By Bayes' rule, the probability of a certain set of category win rates being correct is proportional to its likelihood time the prior. In this case, the likelihood is </p> \\[ \\prod_c \\phi (\\frac{w^*_c - w_c}{\\epsilon_a}) \\] <p>And the prior probability is </p> \\[ \\phi \\left( \\frac{\\frac{ \\sum_c \\left( w^*_c - \\frac{1}{2} \\right)}{n}}{\\epsilon_b} \\right) =  \\phi \\left( \\frac{ \\sum_c \\left( w^*_c - \\frac{1}{2} \\right)}{\\epsilon_b n} \\right) \\] <p>Multiplying them together yeilds </p> \\[ \\left[ \\prod_c \\phi \\left(\\frac{w^*_c - w_c}{\\epsilon_a} \\right) \\right] \\left[ \\phi \\left(\\frac{ \\sum_c \\left( w^*_c - \\frac{1}{2} \\right)}{\\epsilon_b n } \\right) \\right] \\] <p>We are only interested in what has the maximal likelihood, not what that likelihood is. So it is fine to convert this to log odds, which are </p> \\[ \\left[ \\sum_c \\left(\\frac{w^*_c - w_c}{\\epsilon_a} \\right)^2 \\right] +  \\left(\\frac{ \\sum_c \\left( w^*_c - \\frac{1}{2} \\right)}{\\epsilon_b n} \\right)^2  \\] <p>To optimize this, we set the derivative to zero. Applying the chain rule for category d- </p> \\[ 0 = 2 \\left(\\frac{w^*_d - w_d}{\\epsilon_a} \\right) \\frac{1}{\\epsilon_a} + 2 \\left(\\frac{ \\sum_c \\left( w^*_c - \\frac{1}{2} \\right)}{\\epsilon_b n} \\right) \\frac{1}{\\epsilon_b n} \\] <p>Isolating \\(w^*_d\\)- </p> \\[ 2 \\left(\\frac{w_d}{\\epsilon_a^2} \\right) - 2 \\left(\\frac{ \\sum_{c \\neq d}  \\left( w^*_c \\right) - \\frac{n}{2}}{\\epsilon_b n} \\right) \\frac{1}{\\epsilon_b n}= 2 \\left(\\frac{w^*_d}{\\epsilon_a} \\right) \\frac{1}{\\epsilon_a} + 2 \\frac{w^*_d}{\\epsilon_b^2 n^2} \\] \\[ 2 \\left(\\frac{w_d}{\\epsilon_a^2} \\right) - 2 \\left(\\frac{ \\sum_{c \\neq d}  \\left( w^*_c \\right) - \\frac{n}{2}}{\\epsilon_b n} \\right) \\frac{1}{\\epsilon_b n}= w^*_d \\left( 2 \\frac{1}{\\epsilon_a^2} + 2 \\frac{1}{\\epsilon_b^2 n^2} \\right)  \\] <p>So </p> \\[ w^*_d = \\frac{\\frac{w_d}{\\epsilon_a^2} - \\left(\\frac{ \\sum_{c \\neq d}  \\left( w^*_c \\right) - \\frac{n}{2}}{\\epsilon_b^2 n^2} \\right) }{\\frac{1}{\\epsilon_a^2} + \\frac{1}{\\epsilon_b^2 n^2}} \\] <p>With \\(\\beth = \\frac{\\epsilon_a^2}{\\epsilon_b^2}\\), this is </p> \\[ w^*_d = \\frac{w_d - \\beth \\left(\\frac{ \\sum_{c \\neq d}  \\left( w^*_c \\right) - \\frac{n}{2}}{ n^2} \\right) }{1 + \\frac{\\beth}{ n^2}} \\] <p>This expression is the best for gleaning intution behind the adjustment. When the average win rate is high, a larger quantity is subtracted out from all the win rates. If the win rates are all 50%, the numerator becomes \\(\\frac{1}{2} + \\frac{\\beth}{2n}\\), cancelling with the denominator and keeping win rates constant. Higher values of \\(\\beth\\) increase the importance of the distortion term and decrease the importance of the original win rate.</p> <p>While being relatively interpretable, this expression unfortunately cannot be used directly because all of the \\(w^*_c\\) values are unknowns. Some linear algebra is required with the vector forms of \\(w\\) and \\(w^*\\). </p> <p>With \\(J\\) as matrix with \\(0\\) on all diagonals and \\(1\\) on all non-diagonals, the equation can be written </p> \\[ w^* = \\frac{w - \\frac{\\beth J_{n \\times n} w^*}{n^2} + \\frac{\\beth}{2n}\\mathbf{1}_n }{\\left( 1 + \\frac{\\beth}{ n^2} \\right)} \\] <p>Or </p> \\[ \\left( 1 + \\frac{\\beth}{ n^2}\\right) I_{n \\times n} w^* = w - \\frac{\\beth J_{n \\times n} w^*}{n^2} + \\frac{\\beth}{2n} \\mathbf{1}_n \\] <p>Isolating \\(w^*\\) yields </p> \\[ \\left[ \\left( 1 + \\frac{\\beth}{ n^2}\\right) I_{n \\times n} + \\frac{\\beth}{ n^2}  J_{n \\times n} \\right] w^* = w + \\frac{\\beth}{2n} \\mathbf{1}_n \\] <p>The \\(J\\) can be simplified out </p> \\[ \\left[ I_{n \\times n} + \\frac{\\beth}{ n^2}\\mathbf{1}_{n \\times n}  \\right] w^* = w + \\frac{\\beth}{2n} \\mathbf{1}_n \\] <p>Finally, the matrix can be inverted to yield an expression for \\(w^*\\)</p> \\[ w^* = \\left[ I_{n \\times n} + \\frac{\\beth}{ n^2}\\mathbf{1}_{n \\times n}  \\right]^{-1} \\left[ w + \\frac{\\beth}{2n} \\mathbf{1}_n \\right ] \\]"},{"location":"roto/","title":"Rotisserie","text":"<p>The Rotisserie format is significantly different from the two Head-to-Head formats, necessitating some adaptations.</p> <p>One simple way in which Rotisserie is different from the other formats is that its scoring period is a full season, instead of a week. That means season-to-season uncertainty is what is important, rather than week-to-week uncertainty. </p> <p>The other difference is that Rotisserie requires winning against many managers simultaneously, which adds a layer of complication for H-scoring. An objective function to plug into H-scoring for Rotisserie is described in the third paper. See the brief math section below for a brief explanation of the paper.</p>"},{"location":"roto/#variance-in-player-performances","title":"Variance in player performances","text":"<p>Variance in player performances is a key input to the G-score calculation. That means week-to-week variance in the case of Head-to-Head, and uncertainty in season-long performance in the case of Rotisserie. The same quantity is also relevant to H-scoring. While week-to-week variance is relatively simple to estimate based on historical data, pre-season uncertainty is harder to quantify and not studied thoroughly. That creates an issue for using G-scores for Rotisserie in practice.</p> <p>The website's way of handling this is to use scaled week-to-week variance as a proxy for seasonal uncertainty. The \u03c7 factor, which defaults to 60%, controls the degree of scaling. It is one of the input parameters.</p> <p>More technically: the assumption is that the variance over the ~20 weeks in a season will be \u03c7 times the week-to-week variance times 20. If week-to-week variance was the only source of variance, \u03c7 would be effectively 22%. It is likely higher than that before the season, because there is uncertainty about rotations, playing time, offseason improvements, etc. 60% is an estimate with essentially no justification, it can be changed as desired. </p>"},{"location":"roto/#h-scores","title":"H-scores","text":""},{"location":"roto/#website-interface","title":"Website interface","text":"<p>Because Rotisserie is based on fantasy point totals instead of individual match-up victory probabilities, the H-score display for Rotisserie shows expected category totals instead of average matchup winning probabilities. </p> <p></p> <p>Top Rotisserie H-scores, for the 2024-25 season</p> <p>There are a few things to note about this display. </p> <p>One is that since overall H-scores in the Rotisserie context are the probability of winning an entire league rather than a single matchup, they tend to be much lower. The average is around 8% instead of 50%. </p> <p>Another is that adding up the number of expected fantasy points will lead to a total that appears underwhelming, and unlikely to be enough to win any league. That is because the average outcome is almost always going to be far from enough to actually win. The algorithm is banking on some degree of good luck to have any chance of winning, and that is not reflected in the expected values. </p>"},{"location":"roto/#general-strategy","title":"General strategy","text":"<p>It is conventional wisdom that punting is a bad idea in Rotisserie, because winning the entire league essentially requires strong performance in every category. The H-scoring algorithm for Rotisserie largely bears this out, only ever punting Free Throw % for which some players are extreme negative outliers. </p> <p></p> <p>Image from the paper. \u03c7 set to 50%</p> <p>For the most part, the algorithm aims to be moderately strong in all categories. It does tend to deprioritize volatile categories like Turnovers, though only slightly. It wants to win them, it is just banking on some degree of luck to win those categories, instead of investing heavily in them. It chooses to bank on luck in those categories because they are more volatile than the others, and therefore a small amount of luck will go a longer way.</p>"},{"location":"roto/#brief-math","title":"Brief math","text":"<p>Warning- math  </p> <p>The logic behind the Rotisserie algorithm is too complicated to go into fully here</p> <p></p> <p>Too many symbols</p> <p>But here's the basic idea: </p> <p>Say that a team will score \\(Z\\) fantasy points. The expected fantasy point total for an arbitrary opponent algebraically must then be \\(\\frac{P-Z}{|O|}\\) where \\(P\\) is the total points up for grabs, and \\(|O|\\) is the number of opponents. </p> <p>Say also that the standard deviation of total fantasy points for an arbitrary opponent is known, and can be used to estimate how much better the best opponent will do than the average opponent. Call that value \\(L\\). </p> <p>Then, for a given \\(Z\\), the expected difference between the team and its strongest opponent is \\(Z - \\frac{P-Z}{|O|} - L\\), or \\(\\frac{Z(|O| - 1)}{|O|} - \\frac{P}{|O|} - L\\). The overall expected value is \\(\\frac{E(Z)(|O| - 1)}{|O|} - \\frac{P}{|O|} - L\\)</p> <p>Some additional math can estimate the standard deviation for the difference. Then, it is possible to invoke the Normal CDF to estimate the probability of the difference being positive, which is the team's victory probability. That provides an outer-level objective function for H-scoring. </p> <p>This framework provides some intution for why the Rotisserie algorithm tries not to punt categories. The expected value of the difference is generally negative, because it compares the team to its luckiest opponent. The CDF of a negative value of a Normal distiribution will be much higher if the standard deviation is high, since that increases the probability of abberant values. Therefore, the algorithm prefers to have a highly volatile fantasy point total. Punting decreases volatility. It increases confidence in the end result by determining that one category will be lost, and others will likely be won. A balanced team is more volatile, because it could do badly in everything or well in anything. That's the kind of volatility needed to have a strong chance of winning in Rotisserie. </p>"},{"location":"season/","title":"Season Mode","text":""},{"location":"season/#waiver-wire-free-agents-tab","title":"Waiver wire &amp; free agents tab","text":"<p>The waiver wire tab evaluates whether an available player might fit better on an existing team than one of the players already on the team.</p> <p></p> <p>Substitution H-scores for a synthetic team with Derrick Jones Jr. on it, based on the 2024-25 season</p> <p>The player who is a candidate to be dropped is removed from the team, and H-scores are calculated for all available players plus the drop candidate. The drop candidate is highlighted in blue. Players who do not fit the position structure of the team are filtered out and their H-scores are not shown. </p> <p>These H-scores are relatively simple to calculate, because all other players are known and there is no need to strategize around future draft picks. For that reason, the algorithm does not iterate at all and results are shown immediately. The H-score details tab would not be relevant and is not provided. </p> <p>A corresponding view is available for G-scores. </p> <p></p> <p>G-scores for available players compared to Derrick Jones Jr., based on the 2024-25 season</p> <p>The G-scores shown are the scores of the individual players, not what the scores would be for the team if that player was added. </p>"},{"location":"season/#trading-tab","title":"Trading tab","text":"<p>The trading tab analyzes the H-score and G-score implications of potential trades. It also provides recommendations for trades. </p>"},{"location":"season/#trade-analysis","title":"Trade analysis","text":"<p>The trade analysis module analyzes trades proposed by the user. </p> <p></p> <p>The thumbs on the H-score tab for 'Your Team' and 'Their Team' indicate whether a trade improves a team's H-score or not. Thumbs up means the trade is beneficial, thumbs down means the trade is not beneficial. This can also be seen by whether the H-score is higher before or after the trade. </p> <p>Trades in which the same number of players is sent and received are relatively simple to analyze. First, players are switched, then both teams are checked for position structure. If either team is ineligible, then no results will be shown. Otherwise, H-scores are recomputed and compared against the previous H-scores. </p> <p>Asymmetrical trades can also be analyzed, though the methodology is more complicated and less reliable. The post-trade team that goes down in number of players is scored with the normal H-scoring algorithm, which chooses one candidate from the pool of available players and generates a future draft strategy if needed. The post-trade team that goes up in players is scored by checking every possible set of players that could be dropped and finding the option that maximizes H-score. The players chosen for addition/removal through these calculations are not shown. </p> <p>Because a player gets added during an asymmetrical trade, it is important for asymmetrical trade analysis that the list of available players is accurate. A valuable player that appears to be on the waiver wire will artificially make any trade that goes down in players look beneficial, because it allows for that player to be added. Another consideration is that highly assymetrical trades can take time to process because every possible combination of players to be dropped is being analyzed. </p> <p>A G-score table is also provided, which shows the net changes in G-scores for both teams by category.</p> <p></p> <p>This view is available even if the trade is impermissible by position structure. </p>"},{"location":"season/#trade-suggestions","title":"Trade suggestions","text":"<p>Below the trade analysis module, trade suggestions are shown. </p> <p></p> <p>Which trades end up being shown as suggestions depends on the user-configurable trade parameters. </p> <p>Candidate trades are found by first heuristically estimating which players might be more favored by the other team, then iterating through all combinations of possible trades with those players. Those trades are further filtered by a general value difference threshold, which is based on user inputs.</p> <p></p> <p>The general value thresholds limit candidate trades to those between collections of players whose total general values are similar to each other. If the absolute value of the difference in total general value between the two groups of players that are to be traded is above the threshold, the trade will not be considered for analysis. This is to prevent unnecessary computation checking trades that are unlikely to be viable. </p> <p>General value is defined by base H-score (the H-scores the players would have for the first pick of the draft), as a percent. So if the general value threshold is 10, a total base H-score difference of 10% between players is allowed. For asymmetrical trades, the general value of an empty slot is the replacement-level base H-score. </p> <p>Thresholds should generally be lower for trade configurations with larger numbers of players involved, because those require checking many more combinations of players. Decreasing the limit and excluding more potential trades limits computation time. </p> <p>Trade suggestions can be generated for any kind of trade for which a general strength difference threshold is provided. By default 1x1, 2x2, and 3x3 trades are included, and users can add more through the trade parameters popover. </p> <p>After trades are analyzed for H-score implications, one more filter is applied. Only those which meet the H-score differential thresholds as supplied by the user are shown. </p> <p></p> <p>The thresholds are in terms of H-score as a percentage, so if the threshold for a party is -0.2, then trades that decrease the party's H-score by more than 0.2% are not shown. </p> <p>Even with all this filtering, there can be many possible trades to look through, especially when looking for trades with large numbers of players and when the parameters for acceptable trades are loose. For the purpose of limiting computation time, only 1x1 trades are searched for by default. </p>"},{"location":"season/#rosters-tab","title":"Rosters tab","text":""},{"location":"season/#roster-table","title":"Roster table","text":"<p>Rosters can be manually input or edited on this tab. It is unneccesary if rosters are loaded through a platform integration.</p> <p></p> <p>Information from the roster table will only flow through to other components after the 'Lock in' button is pressed. This is to prevent the website from continually updating itself while many players are being added at the same time. </p> <p>Only players from the loaded dataset can be added to the roster table, which are shown on a searchable drop-down for each cell. The same player can be added multiple times.</p> <p></p> <p>Nikola Jokic is still shown as an option after already being taken by another team</p> <p>Generally draft results can be copy-pasted from the drafting view into an Excel and then into this table, so long as the dataset of valid players remains the same. Occasionally there are bugs with copy-pasting into the rosters table, such as the first column not being copied over. In that case, the few remaining players can be entered by hand. </p>"},{"location":"season/#roster-inspection","title":"Roster inspection","text":"<p>Individual teams can be analyzed in terms of G-score. H-scores are not provided in this view. </p>"},{"location":"settings/","title":"Settings","text":"<p>Settings are available for live connections to fantasy providers, customizable projection uploads, and adapting to different kinds of scoring. They are accessible via the left sidebar.</p>"},{"location":"settings/#league-settings","title":"League settings","text":""},{"location":"settings/#fantasy-sport","title":"Fantasy sport","text":"<p>For now, this will just be NBA. MLB or WNBA may be added in the future</p>"},{"location":"settings/#data-integration","title":"Data integration","text":"<p>The default option for getting data on drafting situation/teams is entering it manually. </p> <p>There is also an option for integrating with a fantasy provider, which allows the website to be used with real fantasy occuring on those platforms. The website will show analysis based on the integrated league. It will never make a pick or take any action itself. </p> <p>The three platforms currently supported are: </p> <p>Yahoo: support exists both for pulling existing teams during the season, and for integrating with drafts. This includes mock drafts. To integrate, one must authenticate with Yahoo by following the link on the pop-up generated when Yahoo is selected.</p> <p></p> <p>Once the connection is established, relevant leagues will show up, if any. Mock drafts can also be connected to via manually copy-pasting the code for the mock draft.</p> <p>FYI there is a bug in the wrapper used for connecting to the Yahoo API, which crashes the app when a drafter has a name that is a pure number. </p> <p></p> <p>Fantrax: support exists both for pulling existing teams during the season, and for integrating with drafts. However this only works with public drafts. </p> <p></p> <p>ESPN: support only exists for pulling existing teams during the season. Unfortunately, ESPN has no API for draft access. To authenticate to pull a team, a web plug-in is needed. The instructions are on the pop-up generated when ESPN is selected. </p> <p></p>"},{"location":"settings/#mode","title":"Mode","text":"<p>There are three available modes. </p> <p>Draft mode: for standard drafts. When drafted players are input manually, only snake drafts are naturally supported. If the drafted players are being pulled via an integration, the website will continually evaluate the best prospects regardless of draft position, so any drafting order will work fine. </p> <p>Auction mode: for auction drafts. Users must specify that they are doing an auction for either manual entry or platform integration.</p> <p>Season mode: for leagues where teams have already been built. Season mode allows for the inspection of potential free agents, and also includes a trading module. </p>"},{"location":"settings/#other-info","title":"Other info","text":"<p>If draft data is being input manually, the number of drafters, the number of picks per drafter, and the team names of the drafters are also inputs. </p>"},{"location":"settings/#player-statistics","title":"Player statistics","text":"<p>Player statistics are an input to the algorithms implemented by the website. They can be sourced either from forward-looking projections or previous NBA seasons.</p>"},{"location":"settings/#projection","title":"Projection","text":"<p>The default for player statistics is to use forward-looking projections. </p> <p></p> <p>The default projection source is a 50/50 split between ESPN's free forecasts and a modified version of DARKO. The website's version of DARKO projections takes games played and total minutes from the ESPN forecasts, and combines those with DARKO pace and per-possession projections to get per-game projections. This is necessary because DARKO does not forecast games played, and its minute forecasts are designed for the next game only, which is not ideal for fantasy. </p> <p>Note as of December 2025: the ESPN forecasting page currently has bugs, and for that reason the ESPN projections have not been updated since October.</p> <p>Two additional kinds of forecasts are also available: those created by Hashtag Basketball, and Basketball Monster (BBM). Both of these projections are paid products, so they cannot be provided through the website. Instead, they must be purchased and uploaded. For HTB, there is no native download option for projections. The projections must be copy pasted into an Excel file and saved as a CSV. For BBM, there is a download option, but only the XLSX download option works. Download it, copy out the projections into Excel, and save them as a CSV UTF-8. Either kind of projection can be manually edited to change projections if desired. </p> <p>Also: be careful to download projections for all players instead of just the top players. During a draft, another drafter may take a player outside of the limited projection list, and the website will only have projections for them if they have been provided. </p> <p>Projections are combined between different sources by taking weighted means according to the provided weights. If the assigned weights add up to more or less than 1, they will be scaled to add to 1. </p>"},{"location":"settings/#historical","title":"Historical","text":"<p>Historical data is available for manual entry drafts. </p> <p></p> <p>Historical data is available going all the way back to the 1984-85 season, though for any season before 2000-01 player positions will not be available. </p> <p></p> <p>H-scores for the 1984-85 season, Each Category. NP means no position</p> <p>Historical data cannot be used when integrated with a fantasy platform, because platforms do not run leagues based on past seasons.  </p>"},{"location":"settings/#formats-categories","title":"Formats &amp; categories","text":"<p>The website supports the three common category formats for fantasy basketball: H2H Each Category, H2H Most Categories, and Rotisserie. It does not have native support any additional variants, like using a specific category as a tiebreaker. </p> <p>It also supports any combination of categories, across the default nine categories and several alternative options.</p> <p></p> <p>For the alternative categories, when using projections, make sure to include them when sourcing the projections. ESPN and DARKO do not forecast them so all of the weight will be from Hashtag or BBM projections. </p>"}]}